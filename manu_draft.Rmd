---
title: "The importance of open science for biological assessment"
output: 
  bookdown::word_document2:
    reference_docx: my_styles.docx
bibliography: refs.bib
author: "Marcus W. Beck (marcusb@sccwrp.org), Casey O'Hara (ohara@nceas.ucsb.edu), Julia Stewart Lowndes (lowndes@nceas.ucsb.edu), Raphael D. Mazor (raphaelm@sccwrp.org), Susanna T. Theroux (susannat@sccwrp.org), David J. Gillett (davidg@sccwrp.org), Belize Lane (belize.lane@usu.edu), Greg Gearheart (greg.gearheart@waterboards.ca.gov)"
urlcolor: blue
csl: freshwater-science.csl
link-citations: true
---

```{r setup, echo = F, warning = F, message = F, results = 'hide'}
# figure path, chunk options
knitr::opts_chunk$set(fig.path = 'figs/', warning = F, message = F, echo = F, cache = T, dev.args = list(family = 'serif'), dpi = 300, warning = F, cache.path = 'manu_draft_cache/',
  fig.process = function(x) {
  x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
  if (file.rename(x, x2)) x2 else x
  })

# libraries
library(Jabbrev)
library(tidyverse)
library(gridExtra)
library(raster)
library(sp)
library(sf)
library(leaflet)
library(RColorBrewer)

# functions (incl. color palettes)
source('R/funcs.R')

data(calipsa)
data(scrs)
data(sgrlu)
data(shed)
data(spat)

# extract bib entries from online
bib_scrp('manu_draft.Rmd', 'refs.bib')
```

```{r echo = F, cache = F}
raw <- system('git log -1', intern = TRUE)
raw <- raw[grep('^Date', raw)]
raw <- paste('Version', raw)
```
`r raw`

```{r echo = F, cache = F, eval = F}
spelling::spell_check_files('manu_draft.Rmd')
```

# Abstract 

Open science principles that seek to democratize science can effectively bridge the gap between researchers and environmental managers.  However, widespread adoption has yet to gain traction for the development and application of bioassessment methods.  At the core of this philosophy is the concept that research should be reproducible and transparent, in addition to having long-term value through effective data preservation and sharing.  In this paper, we review core open science concepts that have recently been adopted in the ecological sciences and emphasize how adoption can benefit the field of bioassessment for both prescriptive condition assessments and proactive applications that inform environmental management.  An example from the state of California demonstrates effective adoption of open science principles through data stewardship, reproducible research, and engagement of stakeholders with multimedia applications.  We also discuss technical, sociocultural, and institutional challenges for adopting open science, including practical approaches for overcoming these hurdles in bioassessment applications.

# Introduction

Bioassessment is an essential element of aquatic monitoring programs and helps to establish a foundation of decisions for managing the ecological integrity of environmental resources.  Legal mandates to assess biological condition have set a precedent for developing bioassessment methods in the United States (Clean Water Act, CWA), Canada (Canada Waters Act), and Europe (Water Framework Directive).  Decades of research to meet these mandates have supported the development of methods for multiple assemblages with regional applications in streams, rivers, lakes, and marine environments [@Karr86;@Kerans94;@Fore02;@Beck09;@Borja09].  This body of applied tools represents substantial progress in understanding how biological organisms can be used as accurate and interpretable sentinels of environmental condition.  Monitoring programs in the United States and internationally have collected millions of records of biological data spanning decades and hundreds of assessment methods have been developed from these data. The explicit link to environmental management distinguishes bioassessment from basic ecological research.  Although bioassessment can and has been used to inform basic research, the intended use of these tools is to inform the protection and restoration of ecological integrity.  As such, environmental managers require additional tools that transform bioassessment data and methods into actionable information.  

Bioassessment products will have limited use if they do not meet the needs of management and regulatory communities [@Bain00; @Stein09; @Kuehne17].  In the United States, the CWA gives power to states, tribes, and territories for method development, which in turn requires federal approval to be implemented in a regulatory framework (e.g., Total Maximum Daily Load reporting, stormwater permitting). Currently, there is an imbalance between the methods developed for bioassessment and those that are approved for regulatory use.  A recent review of assessment methods for ecological integrity in the United States showed that few were explicitly connected to freshwater policy [@Kuehne17]. A more problematic issue is the application of bioassessment methods within standard regulatory frameworks; developed tools, although technically sound, are implicitly being used to document the long-term demise of environmental health, after an impact has already occurred. Biological indices are typically used to develop post-hoc diagnoses to trigger remediation or restoration actions, or to serve as early warning indicators of environmental change [@Niemi04].  Bioassessment tools for guiding planning activities, such as identifying conservation priorities [@Linke11;@Howard18], could extend the application of bioassessment products beyond regulatory contexts.  To these ends, the discoverability of existing products by other research teams and the broader management community is essential to minimize duplication of effort and to foster collaboration towards shared goals [@Nielsen11;@Hampton15].   

An effective bioassessment product must jointly address the technical challenges of developing a diagnostic index and the implementation challenges of using an index to address the needs of environmental managers and regulators [@Jackson94;@Dale01]. For decades, research in the bioassessment community has focused on addressing the former.
Substantial technical advances have been made in predicting biological responses to environmental change, how these responses can be distinguished from natural environmental variation, and determining the impacts of these changes.  Standardized protocols for sampling and producing assessment tools have also been developed [@McDonald04;@Stoddard08].  A more urgent need is to address implementation challenges associated with how developed bioassessment products can be integrated into management or regulatory frameworks.  Characterizing how an index could be used in practice to inform decisions and prioritize management actions is often opaque relative to why an index may have been originally developed. Hundreds of assessment methods have been developed for specific regional applications [@Birk12] and concerns about redundancy, comparability, duplicated effort, and lack of coordinated monitoring have recently been discussed within the research community [@Cao11;@Poikane14;@Kelly16;@Nichols16]. Moreover, existing methods may not be discoverable beyond immediate research applications [@Hering10;@Nichols16] or may be incorrectly applied based on differences between goals for developing an index and the needs of management programs [@Dale01;@Stein09].  The abundance of available methods can be a point of frustration for managers given a lack of guidance for choosing an appropriate method among alternatives, particularly as to how a method may relate to specific management, monitoring, or policy objectives [@Dale01;@Stein09].  

To address these challenges, a new mode of operation is needed where method development is open and transparent, developed products are discoverable and reproducible, and most importantly, implementation in the management community is intuitive and purposeful.  Open science principles that democratize all aspects of the scientific method can meet these needs, yet bioassessment research and its application to better serve the environment has not yet fully embraced these principles. Others have advocated more broadly for inclusion of open science principles in the ecological sciences [@Hampton15;@Hampton16;@Lowndes17] and a growing wave of momentum has influenced how scientists conceptualize research in other disciplines [e.g., archaeology, @Marwick16; behavioral ecology, @Ihle17; vegetation sciences, @Collins16].  Adopting an open science paradigm in bioassessment is particularly relevant compared to other fields given the explicit need to develop tools that are accessible to the management community. Legal and ethical precedents in bioassessment may also necessitate open data sharing given that environmental monitoring programs are often publicly funded.

This review demonstrates tools and approaches for open science, which will empower the research and management community to embrace a new mode of thinking for bioassessment applications. These approaches are expected to benefit the bioassessment research community by augmenting existing workflows for developing assessment tools and improve the ability of these methods to address environmental issues by bridging the gap between the scientific, management, and regulatory communities. As such, this paper describes open science tools that can be adopted in research practices with an emphasis on communication, but we also stress the value that these tools have for the broader community of environmental managers. An overview of the general principles of open science is provided, followed by a discussion of specific benefits and how these principles can be applied to bioassessment.  We use examples from the state of California to demonstrate how existing tools can be tailored to address legislative mandates for free and open sharing of data, especially by directly engaging stakeholders that require practical approaches for using bioassessment tools in planning activities.  We conclude with a discussion of technical, sociocultural, and institutional hurdles that have, thus far, prevented widespread adoption of open science and provide recommendations for the bioassessment community to address these challenges.

# Principles of open science

Conventional modes of creating scientific products and more contemporary approaches that align with open science principles share the same goals.  Both are motivated by guiding principles of the scientific method that make the process of discovery transparent and repeatable. Where the conventional and open science approaches diverge is the extent to which technological advances are leveraged as instrumental tools that facilitate the entire research process.  Distinction between the two approaches can be conceptualized as the "research paper as the only and final product" for the conventional approach, whereas the open science approach is inherently linked to advances in communication and analysis that have been facilitated by the Internet and computer sciences (Table \@ref(tab:osprinc)).  As a result, the open science approach can enhance all aspects of the scientific process from initial conception of a research idea to the delivery and longevity of a research product (Figure \@ref(fig:basicflow)).  The process is iterative where products are improved by the individual and/or others, facilitated by open science tools that enhance access and reproducibility of data. 

The paradigm of the research paper as a final scientific product can inhibit the uptake of methods and findings by environmental managers.  The research paper is conventionally viewed as a communication tool for scientists to report and share results among peers.  Researchers access periodicals to stay informed of scientific advances and use the information to replicate methods for follow-up analysis.  Although the primary literature continues to provide these fundamental services, this workflow is problematic when scientific products are needed to serve interests outside of the research community.  For example, the paper as an endpoint for environmental managers fails to deliver tools that are easily accessible from the practitioners perspective, both in application and interpretation.  A research paper is less likely to effect environmental change because it does not provide a mechanism to transfer actionable information to those that require scientific guidance for decision-making, such as sharing analysis code or results that describe output from assessment tools. Numerous studies have documented implementation failures as a result of siloing among research communities where the flow of information does not extend beyond institutional walls [e.g., @Mitchell05;@Liu08].  Information loss over time is another well-known flaw associated with the paradigm of research paper as final product [@Michener97].

## Open data as a component of the open science process

Open data is a fundamental component of the broader open science process in Figure \@ref(fig:basicflow). Under this mode of thinking, the research team becomes stewards of its data. The responsibility for data stewardship also extends beyond the research arena to public institutions that maintain large datasets or synthesize smaller datasets from multiple sources. Stewardship allows the data to be treated as a living product with a traceable and replicable provenance (i.e., origin), rather than proprietary and serving only the internal needs of an immediate research goal. Metadata that describe the structure and history of a dataset are essential tools that ensure the data have an identity. Metadata also encourage adoption of core data structures that allow integration across different sources, which is critical for collaboration across institutional boundaries [@Horsburgh16;@Hsu17].  Other open science practices, such as integration of data with dynamic reporting tools or submitting data to a federated repository (i.e., a decentralized database system for coordination and sharing), can be applied to any and all of these data with the end goal of facilitating communication for researchers and those for which the research was developed [@BondLamberty16]. These tools also benefit the individual researcher by providing information and version tracking to recreate a past working environment. In all cases, the data are openly accessible and documented for reproducibility and discovery using technologies that facilitate communication and sharing.  

Open data can benefit research by contributing to an increase in novel products created through collaboration.  Collaborative publications have increased in the environmental sciences as research teams leverage open data to create synthesis products as the sum of individual datasets. Quantitative meta-analyses and systematic reviews are increasingly used to extract information from the primary literature [@Lortie14].  In addition, open data products can increase efficiency of the individual researcher and a collective research team by encouraging collaborators to adopt an open science workflow.  Many tools developed within the software and computer science community to facilitate open process and the creation of open data are now easily accessible to environmental scientists.  Version control software (e.g, Git, GitHub), open source programming languages (e.g, R, Python), and integrated development environments (IDEs, e.g., RStudio, Spyder) can all be leveraged by applied ecologists to dynamically create and share open data products that build institutional memory.  These tools promote deliberate and shared workflows among researchers that can lead to better science in less time [@Lowndes17] and have proven useful in recent applications in the hydrologic sciences [@Idaszak17].

Open access to data can also benefit management and regulatory communities.  Although many monitoring datasets are publicly available, the data quality, level of documentation, and ease of use varies widely.  This can create research and implementation challenges as datasets may be difficult to locate, data contexts may be misunderstood, and data from different sources require synthesis.  Many open science tools can improve the accessibility of data from monitoring programs by establishing workflows for data discovery and synthesis, often through the adoption of a common metadata structure (e.g., Ecological Metadata Language Standard, or [EML](https://knb.ecoinformatics.org/external//emlparser/docs/index.html)) and integration of data within federated data networks (e.g., [DataONE](https://www.dataone.org/), [iRODS](https://irods.org/)).  Open data maintained by management or regulatory communities benefits the research community, which in turn benefits the data maintainers that require scientific products to inform decisions.  Open data can also improve public trust in scientific findings by exposing the underlying information used to develop a research product [@Grand12].  Similar concepts are used in "blockchain" technologies that allow public financial transactions in an open, distributed format, as for trading in cryptocurrencies [@Pilkington16]. Increased trust could facilitate eventual adoption of proposed rules or regulations that are based on research products created from open data.

# Applying open science principles to bioassessment

Here we provide a detailed description of open science processes that the bioassessment community could leverage to adopt a philosophy of creating reproducible, transparent, and discoverable research products for environmental managers.  To emphasize the value of open science for specific steps of the scientific process, we first describe a "conventional" linear process (Figure \@ref(fig:closedopen)a), which is then contrasted with a process that adopts open science tools to facilitate feedback between the management community and stakeholders (Figure \@ref(fig:closedopen)b).  The critical differences are the iterative flow of ideas and products between the management community and stakeholders, the research team developing the bioassessment product, and the broader community that provides data and guidance documents in the primary and secondary literature.  In both cases, the technical and implementation phases of a bioassessment product are acknowledged as distinct steps of analysis that describe the entire process from idea conception to adoption in management or regulatory applications.  We use this paradigm to jointly demonstrate how open science tools can be applied beyond the research phase, but also reinforce the concept that a bioassessment product is only as relevant as its management application (e.g., a bioassessment product will not have applied value if its final home is the primary literature).

Efforts to formally recognize and integrate open science in bioassessment are needed now more than ever.  The transition of bioassessment from taxonomic-based indices to molecular approaches presents novel challenges that will only increase in severity as researchers continue to refine methods for molecular applications [@Baird12].  Although molecular-based indices share similar assessment objectives as conventional indices, the data requirements and taxonomic resolution are substantially more complex.  Bioassessment research teams developing molecular methods are and will continue to be inundated with data from high-throughput DNA sequencers.  Systematic approaches to document, catalog, and share this information will be required to advance and standardize the science.  Molecular approaches are also dependent on existing reference libraries for matching DNA samples for taxonomic identification. The integrity of reference libraries depends greatly on the quality of metadata and documentation for contributed samples.  Open science principles should be leveraged in this emerging arena to ensure that new bioassessment methods continue to have relevance for determining the condition of aquatic resources. 

The below examples require understanding the distinction between the general open science process in Figure \@ref(fig:basicflow), open data as an individual component of the open science process, and the technology-based tools that can be used to achieve these ends.  We emphasize that both the tools and open data are critical components that facilitate the broader process to achieve the principles outlined in Table \@ref(tab:osprinc). However, open science tools can also be used by individual researchers in an entirely closed workflow with no collaboration or discoverability by others.  Similarly, open data can be created through an entirely closed process even though it may appear as an open science product.  "Openness" of process, tools, and data exists on a continuum, and incremental improvements can transform an individual's and research group's practice over time.  We encourage awareness that an open process adopts the open science tools that are appropriate for a research question, the creation of open data can be a fundamental component of the process, and acceptance by the research team and collaborators of the concepts described in Table \@ref(tab:osprinc) is critical to achieving openness.  

Concepts of openness in Table \@ref(tab:osprinc) also have analogs in the conventional paradigm that are worth noting to better distinguish between open and conventional workflows. First, an open process, as noted above, encapsulates concepts of transparency, discoverability, and availability, whereas a closed process is marked by isolation from any external feedback between researchers and outside institutions. Closed processes are also opaque because they are concealed from outsiders and cannot be easily replicated.  A common example is manual spreadsheet manipulation where the steps are neither explicitly nor implicitly documented (as through scripting or pseudocode), such that others and the individual researcher may be unable to replicate the workflow in the future. Further, analysis tools that are created with open source platforms may also be opaque if there is no documentation or description of the underlying analyses to illuminate the "black box" of the internal code. Finally, open processes can make data available both from an accessibility and "readability" perspective, as for both human- and machine-readable formats. Inaccessible data by contrast have none of these properties and may be proprietary or unreadable for analysis (e.g., an image of a data table in a PDF). 

## Conventional bioassessment

First we consider a hypothetical process by which many bioassessment products are created to highlight challenges to openness with conventional approaches.  A typical workflow for developing a bioassessment product is not entirely dissimilar from a conventional scientific process.  The primary investigator begins by identifying the research goal, developing methods to achieve that goal, and then identifying the data needs and analyses based on the existing science.  What distinguishes bioassessment from traditional scientific studies is that research goals are often defined externally through legislative requirements.  This means that the required datasets may be decentralized across institutions or data collection may be contracted to a third party.  This separation between consumers and creators of bioassessment products can be a contributing factor for adopting closed workflows given communication barriers between institutions. For these reasons, the conventional process is often linear and opaque by lack of communication and transparency between parties (Figure \@ref(fig:closedopen)a).

A conventional process for a bioassessment study can also lead to inaccessible data with inadequate documentation. Many bioassessment products begin by synthesizing data from existing monitoring programs, although the original goals for a monitoring program may not align with the needs of a bioassessment product.  The data may also have limitations that affect performance or even prevent their use entirely for assessment.  For example, a fisheries monitoring program may only sample select locations and may have inadequate coverage of aquatic biodiversity.  In these scenarios, supplementary data must be gathered from other sources.  The end product is a synthesized dataset that may be appropriate to meet the requirements of a bioassessment product, but its origin and workflow to achieve the necessary format may be unknown except to the individual that compiled the data.  This creates significant challenges for future products or policies that depend on these products and may even erode institutional trust if the provenance of the dataset is unknown. 

Insufficient tools to facilitate use of bioassessment products by managers and policy-makers, both in calculation and interpretation, represents a critical limitation of conventional workflows.  The worst case scenario is a spreadsheet-oriented approach to converting raw data into a synthesized index.  Not only is this a major challenge for the researcher that develops the index, but its methods and order of operations are opaque, preventing others from applying the tool to reproduce the results and apply it in novel situations.  There are many reasons for these challenges (e.g., lack of documentation, unknown data requirements, no interpretation guidance), but the fundamental issue is inaccessibility of the product. A management or regulatory agency will have no ability to interpret these data and the final product will not pass independent review if the results cannot be reproduced.  Recent workflows have been proposed as "cradle to dashboard" to remediate these issues, where code development, data, and final products are intimately linked from conception to completion (e.g., for environmental flows,  [https://eflows.ucdavis.edu/](https://eflows.ucdavis.edu/hydrology)), but these approaches are still uncommon within the bioassessment community.  

## Using open science tools to enhance bioassessment

The above example is an extreme hypothetical description where bioassessment products fail to effect any positive change in environmental management due to a complete deficiency in openness, resulting in failed implementation.  Many existing bioassessment methods can be described more positively, yet the research community could benefit from adopting a more open approach to creating and delivering assessment tools.  This approach is particularly relevant from the perspective of implementation and science translation, as all modes of communication between research, managers, and regulators could be enhanced with open science.  The following section describes key components of the open science process that can facilitate the development and implementation of bioassessment products.  We focus specifically on applications for data provenance, method development, and method delivery.    

The overall process is shown in Figure \@ref(fig:closedopen)b as an expansion of general concepts in Figure \@ref(fig:basicflow), with a specific science translation phase for implementation.   This iterative flow of information is facilitated by 1) openly sharing planning documents, 2) using established metadata standards to document synthesized data products, 3) hosting data products on open repositories, 4) creating reproducible summary documents that integrate the data and research products, and 5) incorporating the developed product into interactive applications that deliver the results to the managers and stakeholders.  The technical phase of defining research goals, collecting and synthesizing data, and developing the bioassessment product are primary tasks of the research team.  However, the process is distinguished by the flow of information to and from the research phase that can benefit the specific project and the science of bioassessment as a whole.

### Developing bioassessment goals

In an open science process, the goals identified by the research team for developing a bioassessment product should occur through direct, two-way interaction with the management or regulatory institution that requires the product.  This two-way exchange of information can be accomplished through direct communication and sharing of planning documents to ensure all decisions are transparent, i.e., open planning.  In person meetings are ideal, but planning documents are dynamic and will require remote sharing and revision as ideas progress.  Online tools such as [Google documents](https://www.google.com/docs/about/), [Slack](https://slack.com/) discussion channels, and open lab notebooks can be instrumental for collaboration. More informal approaches, such as blogging and sharing ideas on social media, can expose new concepts to the broader community for guidance [@Woelfle11;@Darling13].  Overall, the research team should use these tools to identify stakeholder needs while also considering the balance between the research goals and limitations of the data to meet these goals.   This will ensure that the needs of the management and stakeholder communities will be consistent with the services provided by the research product.  

### Curating bioassessment data

After the goals are established, the research team identifies requirements and sources of data that need to be synthesized to meet the research needs.  Under a closed scenario, data flows one way from the source to the researcher and is used only as a means to create the final research product.  In the open scenario, the data itself is a product to achieve the research goals and also becomes available to the research and management community as a fully documented source of information that has value beyond the specific project.  The openness of the synthesized data product is one of the primary means of facilitating the implementation phase of a bioassessment product.  The synthesized data product can be used by the research team to create interactive applications for stakeholders to share and explore the data and is also fully integrated into summary reports using software for generating dynamic documents [e.g, using `knitr`, @Xie15; RMarkdown, @Allaire18; Jupyter notebooks, @Kluyver16]. The data product also becomes available on an open data repository that is discoverable by other researchers and can contribute to alternative scientific advances beyond the immediate goals (e.g., Hydroshare for the hydrologic sciences, @Idaszak17). 

Open bioassessment data created by a research team using an open science process clearly describes the history and purpose of a dataset to easily allow additional research and delivery of products to managers. This is particularly relevant given that a synthesized dataset can originate from multiple sources and the combined output may differ substantially from the raw data.  Documenting provenance of a data product allows a research team and others to understand the purpose of a combined dataset, how it was created, and how it differs from the original data.  For example, a multimetric index may require taxonomic data collected at multiple sites by different institutions, whereas the output data may include summary scores, individual metrics, and any additional supporting information to assess the quality of the output. A data product can easily be documented using a standardized metadata language (e.g., EML) which describes the who, what, and why to ensure the data have an identity.  Adoption of a metadata standard also ensures that a machine-readable file is produced to allow integration into a data repository.  This will allow a synthesized data product to be discoverable beyond the specific research application and will provide metadata to help others understand the context of the data.  Finally, a dataset can be assigned a unique digital object identifier (DOI, e.g., through [Zenodo](https://zenodo.org/)) that provides a permanent address and is also citable to allow researchers to track usage of a bioassessment data product.  

### Using R for bioassessment translation

The translation of bioassessment products to the management and regulatory community is critical to ensure that research is applied by decision-makers for its intended purpose. Translation should be fundamentally linked to open science analysis and development tools that can be used to deliver the products using a reproducible and accessible platform.  In particular, the R statistical programming language [@RDCT18] is one of the most commonly used analysis platforms in the environmental sciences for creating specific analysis workflows [@Lai19], yet the availability of existing packages and the ability to create new packages is a strength of R that is under-utilized by the bioassessment community. An assessment index packaged in R can automate the tedious process of converting raw taxonomic data to bioassessment information. A well-designed R package is also modular, meaning it includes all necessary analyses, data, and documentation to allow use by others. This modularity is critical for reaching management and stakeholders by providing a tool that redirects the focus on the output and not the technical details that are less important for decision-making.    

Many existing R packages have value for the bioassessment community (Table \@ref(tab:rpkgtab)).  For managing the day to day tasks of working with multiple datasets, the `tidyverse` suite of packages provides the necessary tools to import, wrangle, explore, and plot almost any data type [@Wickham17b].  These packages are developed around the concept of "tidy" data that provide a common and natural framework for working with data [@Wickham14c].  These principles have importance for bioassessment where the synthesized datasets used to create a product should be logically structured to facilitate use by others.  Although the time and effort required to produce a tidy dataset can seem excessive relative to native formats (e.g., raw sample data), downstream analyses will be greatly facilitated because a common format can be anticipated rather than having to develop specific workflows to accommodate raw data.  The `tidyverse` also includes the powerful `ggplot2` package that is based on a syntactical grammar of graphics for plotting [@Wilkinson05;@Wickham09].  This package provides a set of independent plotting geometries and aesthetics that can be built piecewise and is a departure from other graphics packages that represent a collection of special cases that limit the freedom of the analyst.  The structure of `ggplot2` makes data visualization an integral part of the analysis workflow, rather than only at the end of an analysis where final graphics are produced.  In bioassessment, `ggplot2` can be used both in an exploratory role during the development phase and also to create publication quality graphics.  More importantly, this package provides the building blocks to create effective data visualizations that convey important components of a bioassessment product to managers and stakeholders.   

Bioassessment data are inherently spatial and recent package development has greatly improved the ability to analyze and map geospatial data in R.  These tools can readily communicate the spatial context of bioassessment products to managers and stakeholders by mapping index or condition scores to stream flow networks, watersheds, and ecoregions, both for high-level planning and site-specific evaluations.  The `raster` package can used to read/write, manipulate, analyze, and model grid-based spatial data [@Hijmans19], which are often common supporting layers for bioassessment (e.g., elevation or climate data).  For vector data (i.e., points, lines, and polygons), the `sf` package ("simple features", @Pebesma18) was first released in 2016 and has quickly become the most highly-used approach for working with spatial information in R.  The `sf` package uses principles of data storage that parallel those from the `tidyverse` by representing spatial objects in a tidy and tabular format.  This facilitates analysis by presenting complex spatial structures in a readable format that can be integrated in workflows with existing packages, including other mapping packages [e.g., `leaflet`, @Cheng18; or `mapview`, @Appelhans18].  This allows the research team to use a workflow that is focused in a single environment, rather than using separate software for statistical and geospatial analysis.  This facilitates participation if a research team has a shared analysis platform, while also minimizing loss of information or introducing analysis errors between environments.

Several existing R packages can be used to develop statistical models of bioassessment data that are a necessary component of many analyses.  Random forest models have been used to develop predictive bioassessment indices that compare observed taxa to modelled expectations (i.e., O/E indices).  The `randomForest` package [@Liaw02] uses an ensemble learning approach that is robust to complex, non-linear relationships and interactions between variables.  These models are particularly useful with large, regional datasets that describe natural and anthropogenic gradients in condition [@VanderLaan14;@Mazor16].  Many other modelling packages are available in R that can support index development, such as exploratory analyses to evaluate biological response or identifying significant associations of organisms with stressor gradients.  The `nlme` package can be used to create non-linear mixed effect models that are more flexible than standard regression approaches [@Pinheiro18].  The `nlme` package can develop models for nested sampling designs, such as repeat visits to sample sites or otherwise confounding variables that contribute information but are not unique observations [@Mazor14].  The `mgcv` package provides similar functionality as `nlme`, but uses an additive modelling approach where individual effects can be evaluated as the sum of smoothed terms [@Wood17]. The `mgcv` package is often applied to model biological response to stressor gradients [@Yuan04;@Taylor14] 

Other R packages have been developed specifically for bioassessment.  For example, the `TITAN2` package can be used to develop quantitative evidence of taxon-specific changes in abundance and occurrence across environmental gradients [@Baker15].  Results from this package can support exploratory analysis for developing bioassessment products, such as identifying indicator species or establishing numeric criteria [@Taylor18].  The results can be also be used post hoc to evaluate potential response of a biological index with changing environmental conditions, such as proposed management actions for rehabilitation [@King11].  Alternatively, the `indicspecies` package provides similar functionality but is based only on species occurrence or abundance matrices across sites [@DeCaceres09].  This package can be used to identify species that occur at particular sites if continuous environmental data are unavailable, such as those that are representative of reference conditions [@Bried14].  Finally, the `vegan` package has been a staple among community ecologists for multivariate analyses in R, such as clustering and ordination [@Oksanen18]. This package has value for bioassessment as an exploratory tool with a variety of applications, such as analyses that identify comparable assessment sites that minimize natural variation [@Beck13].

Although the R network includes over 10000 user contributed packages, the most under-utilized aspect of this software for bioassessment is the creation of new packages to support applied research. Community practices have allowed R to reach new audiences where new packages build on the work of others and are transportable between users and operating systems, rather than all researchers reinventing the wheel through duplicated effort.  Formalized communities, such as [rOpenSci](https://ropensci.org/), encourage standardization and review of contributed packages within the ecological sciences to make scientific data retrieval reproducible.  Several tools have also been developed and published in the last five years that greatly simplify the process of creating new packages in R [@Wickham15;@Wickham18].  The increasing popularity in the environmental sciences of online repositories for hosting code and software, such as GitHub, also provides a venue for sharing and directly installing these packages.  

The advantages of creating and sharing R packages that are specific to bioassessment applications are important for several reasons.  First, a well-designed R package provides a compartmentalized set of instructions developed during the technical research phase that can be executed by anyone with access to the software.  This allows the developer to include important technical elements required for the execution of a bioassessment product within the package, while allowing the end user to focus on how the output can be used to inform decision-making.  R packages also require explicit documentation of the functions and data requirements.  As such, package users will not only have access to underlying code but also understand the why and what for different package functions.  Detailed vignettes can be included in plain language to describe how to use the package. 

Peer-review mechanisms are available for user-contributed packages, such as through rOpenSci or through package-oriented journals (e.g., the R Journal, The Journal of Statistical Software).  These review mechanisms are meant to ensure that the underlying code and algorithms are satisfactory for the stated needs that are addressed by the package. However, a majority of user-contributed packages (e.g., on GitHub) are not formally reviewed and users are encouraged to always assess the validity of a package before application. Cursory checks can be made by verifying author credentials or conferring among colleagues as to which packages are commonly used for particular analyses. However, software review is the gold standard and package developers are increasingly encouraged to allow external review of underlying code and theory prior to submitting a package to a public repository [@Borregaard16].  

Finally, R can be used to create interactive applications that deliver bioassessment products to stakeholders and managers in entirely novel contexts.  In particular, the `shiny` package was first released for R in 2012 and provides programming tools built around concepts of reactivity, where data inputs and outputs can be modified in real time [@Chang18].  A `shiny` application is an interactive user interface that is developed with R code, but is a standalone product that can be used without any programming experience.  These applications are deployed online and can extend the reach of bioassessment tools to those that require the information for decision-making but otherwise do not have the time or resources to learn R.  Applications built in `shiny` can also be easily linked to other R packages.  For example, a `shiny` website could be created to allow users to upload raw data and estimate and report bioassessment scores using an R package developed externally.  This can extend the accessibility of a bioassessment product while maintaining the technical integrity of the original tool.  Moreover, `shiny` applications are completely customizable and can be tailored by the developer to the specific needs of any user.  This distinction separates `shiny` from other web-based analysis platforms.    

# Open science in practice

Although bioassessment methods have been sufficiently developed in California, there are no narrative or numeric criteria in place to support designated aquatic life uses in wadeable streams, nor are bioassessment data actively used to support conservation or watershed management.  Indices using benthic macroinvertebrates and algae have been developed that provide consistent indications of biological condition across the diverse geography and climates in the state [@Fetscher13;@Ode16;@Mazor16]. A physical habitat index has also been developed that provides complementary information supporting bioassessment data [@Rehn18]. Combined, these indices represent significant achievements in overcoming technical challenges for developing accurate and interpretable bioassessment methods. However, these products are not used at a statewide scale to inform decisions and past efforts for stream management have only used a fraction of available methods.  A synthesis of condition assessments from multiple methods is needed to effectively implement bioassessment methods in California and data must be presented in a context that is relevant to the needs of decision makers.

Recent regulatory initiatives in California (USA) have established a foundation for openness that could greatly improve the application of bioassessment methods to support decision-making.  In particular, these initiatives have set a precedent for openly sharing data collected with public funds. The Open and Transparent Water Data Act passed by the state legislature in 2016 requires water quality institutions to "create, operate, and maintain a statewide integrated water data platform that, among other things, would integrate existing water and ecological data information from multiple databases and provide data on completed water transfers and exchanges" ([AB 1755, Dodd, 2015-2016](https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=201520160AB1755)).  This legislation also calls for state agencies to "develop protocols for data sharing, documentation, quality control, public access, and promotion of open-source platforms and decision support tools related to water data".  These aspirations were further supported by a [resolution](https://www.waterboards.ca.gov/press_room/press_releases/2018/pr_water_data_071018.pdf) on July 10, 2018 that formally committed the State Water Resources Control Board to "provide broader access to data used to make local, regional, and statewide water management and regulatory decisions in California".  These recent initiatives in California have similarly been observed at the national level. For example, the [Data Coalition](https://www.datacoalition.org/) is an advocacy group that operates on behalf of the private and public sector for the publication of government data in a standardized and open format.  The [Internet of Water](https://internetofwater.org/) also operates at the national-level by focusing on strengthening connections between data producers and users through centralized data hubs and data standards.

The recent policies and initiatives are a recognition of the disconnect between the state of the science and the level of integration with policy and decision-making. Although these initiatives represent substantial progress, the implementation of existing bioassessment methods to support policy or planning remains uncertain.  For example, thresholds for bioassessment indices that are protective of ecosystem integrity are often required for application in a regulatory context.  Thresholds can be identified based on variability of reference conditions, yet the justification for use in policy requires a link between the threshold and ecosystem function.  Moreover, reference conditions may be an unreasonable management target in developed landscapes where biological communities could be constrained by stressors originating at the watershed scale [@Paul08;@Kenney12;@Shoredits13]. These constraints represent significant challenges in setting statewide thresholds for bioassessment indices where goals may be easily met in some areas of the state (e.g., open areas) but not others (e.g., urban areas).  

A fundamental concern of regulated parties with the potential adoption of statewide numeric criteria is the likelihood of achieving a one-size-fits-all requirement across the variety of landscapes and channel types in the state [@Lane18].  Although assessment tools have been developed to account for regional variation in stream communities, there is a disconnect between what biological condition is a reasonable management goal and what might be required under a statewide criteria.  For example, a threshold that represents the tenth percentile of bioassessment scores at all statewide reference sites would be relatively easy to achieve in the Sierra Nevada region that is largely undeveloped.  Conversely, a similar threshold would be very difficult to achieve in Southern California where heavy urbanization has occurred in the last seventy years and much of the hydrologic network has been modified through channelization or concrete reinforcement.  Managers require bioassessment tools that not only identify state of the resource, but what is a reasonable expectation for biological condition given the immutable changes on the landscape that are largely beyond management intervention. Should concrete channels be held to the same standard as streams in more natural settings, and if not, what bioassessment tools are available to support management goals in these different environments?   

Open science methods have recently been used in California to address bioassessment implementation challenges in developed landscapes.  The Stream Classification and Priority Explorer or SCAPE [@Beck18c;@Beckir] was developed using an open science framework to help identify reasonable management goals for wadeable streams using existing bioassessment and watershed data.  The SCAPE tool represents both a modelling approach to identify appropriate management goals and a set of open science products for science translation to environmental managers.  First, a statewide landscape model was developed that predicts a range of likely biological condition scores using the California Stream Condition Index [CSCI, @Mazor16] from a suite of watershed metrics (e.g., land cover, road density). The model was developed specifically using predictors that are typically beyond the scope of management intervention to describe the range of scores that are likely to be observed under current land use conditions.  From this information, a stream classification system was developed to identify sites where biological condition is "constrained" or "unconstrained" by landscape alteration.  This classification system can be used to set management priorities based on the constraint class.  For example, a monitoring site with an observed biological index score that is above a predicted range could be assigned a higher management priority relative to a site that is scoring within the range that is expected based on landscape development.  

An example of how the SCAPE tool can link bioassessment information to managers is demonstrated in Figure \@ref(fig:scapexfig).  The model was developed using statewide data, whereas results were regionally applied to develop management priorities through close interaction with a local stakeholder group from the San Gabriel River Watershed (Los Angeles, California).  The San Gabriel watershed represents an extreme landscape gradient with open areas in the upper watershed and heavy urbanization in the lower watershed where almost all channels are engineered for stormwater management or water conveyance.  Streams in the lower watershed represent a management challenge where biological potential is limited by landscape development; reference conditions based on a statewide expectation may be difficult to achieve in these areas.  The landscape model helps identify channels where biological constraints occur as a function of landscape to help prioritize among locations. All stream segments in the watershed were classified as constrained or unconstrained by landscape and bioassessment sites were then evaluated relative to the class and ranges of scores that were expected given the landscape.  Management actions and priorities were then defined based on the stream class and if the observed score was as under-score, as expected, or over-scoring.  For example, sites not scoring as expected were assigned high priority for additional investigation, unconstrained sites were identified as high priority for protection, and under-scoring unconstrained sites were assigned high priority for restoration.  
  
Open science products that were created through the life cycle of the SCAPE tool were critically important for translating and delivering bioassessment data to decision-makers.  Not only does the model address a critical management need (i.e., where are constrained streams and how can they be prioritized), but all aspects of model development, implementation, and translation followed an open science workflow (Figure \@ref(fig:scapeopensci)). Local stakeholder engagement to identify research goals guided the technical development process of SCAPE (Figure \@ref(fig:scapeopensci), box 1). Continued involvement of this group throughout the project ensured the final product addressed all research goals and management needs.  All analyses, including model development and validation, were conducted using the R statistical programming language [@RDCT18].  Additional R packages that greatly facilitated the process included the `tidyverse` suite of packages for data import, wrangling and plotting [@Wickham17b], the `sf` package for spatial analysis [@Pebesma18], and `randomForest` for modelling [@Liaw02] (Figure \@ref(fig:scapeopensci), box 2). A version control system (Git) and online hosting (GitHub) also allowed full transparency of decisions that were made to create the tool.  A permanent DOI was assigned through Zenodo to track downloads and portability of source code (Figure \@ref(fig:scapeopensci), box 3). 

A critical component of the SCAPE tool was the use of an online, interactive web page ([https://sccwrp.shinyapps.io/SCAPE](https://sccwrp.shinyapps.io/SCAPE)) that was developed entirely using open-source software [i.e., R, @RDCT18; `shiny`, @Chang18, Figure \@ref(fig:scapeopensci), box 4)]. This online application provided two fundamental services that greatly increased the impact and relevance of the bioassessment product for the management community.  First, the application improved stakeholder understanding of the technical model components through direct interaction with key decision points that influenced model output.  This improved stakeholder comprehension and trust for the intended application of the model by using a hands-on approach for interacting with the landscape model.  Second, potential decision points that affected model output were not hard-coded into the application so that the effects of varying parameters (e.g., potential biointegrity thresholds) could be better understood relative to management implications.  This second component is a fundamental benefit of open science applications in bioassessment because the tool was developed to allow exploratory analysis, both within the constraints of the model and the needs of the management community.  The online tool did not limit managers to a single answer, rather it provided the tools to determine the most appropriate answer for different situations by giving managers the ability to directly interact with the model.

Conventional scientific products for the SCAPE tool were also created as deliverables using open science tools. These were created as products for both the local management community with direct interests in the landscape model and the broader scientific community concerned with bioassessment challenges in developed landscapes.  A manuscript describing the model in detail was created using `knitr` and `RMarkdown` [@Xie15;@Allaire18].  This increased efficiency of the writing process and also minimized the potential of introducing errors into tables or figures by eliminating the need to copy results between different platforms.  The manuscript was also under full version control through GitHub for streamlined collaboration and tracking of revisions (Figure \@ref(fig:scapeopensci), box 5).  A geospatial data file from the model was also made public on a federated data repository, which included metadata and plain language documentation to track provenance of the original information [@Beck18d] (Figure \@ref(fig:scapeopensci), box 6).  This ensured that the final data could be discovered by others through keyword searches and also provided a platform to track broader interest in the data products through download statistics. 

The SCAPE tool is one example of how open science can be effectively integrated with bioassessment data.  Most importantly, SCAPE is an interactive decision-support tool that allows stakeholders to more effectively use bioassessment data to prioritize management actions on the landscape.  Open science aspects of SCAPE enhanced this functionality by providing a transparent and reproducible product that greatly expanded the utility of existing bioassessment products in California. The open science products described above could be leveraged in other contexts to increase the impact and relevance of bioassessment data for achieving management goals. 

# Limitations and opportunities

Although the case for open science in bioassessment is appealing, the widespread adoption of these principles in practice is inhibited by inertia of existing practices, disciplinary culture, and institutional barriers. Conventional and closed workflows used by many scientists are adopted and entrenched because of ease of use, precedence, and familiarity, yet they can be inefficient, inflexible, and difficult to communicate or replicate. Open science tools can improve analysis, documentation, and implementation through greater flexibility, but they expose research teams to entirely new concepts and skillsets in which they may never have been trained [e.g., @Idaszak17].  Many scientists feel they cannot prioritize learning new skills given existing demands on their time, particularly if the benefits of these approaches, such as the value for the research team of sharing their data, may not be apparent or immediate. Not only are the required skillsets demanding, but the open science toolbox continues to expand as new methods are developed and old methods become obsolete.  This requires a research team to stay abreast of new technologies as they are developed and weigh the tradeoffs of adopting different workflows for different research tasks.  

Advocates for open science are well aware of the technical challenges faced by individuals that have never been exposed to the core concepts.  Most importantly, education and training (e.g., through [The Carpentries](https://carpentries.org), [DataCamp](https://www.datacamp.com/)) remain key components for developing skillsets among researchers where the focus is both on learning new skills for transferability and realizing their value for improving science as a whole [@Hampton17].  A goal of many training curricula is to instill confidence in new users by developing comfort with new workflows, such as replacing a point-and-click style of analysis with one focused on using a command line through a computer terminal. Other approaches to demonstrate the value of new techniques use a side by side approach of closed vs open workflows to show the increased efficiency and power of the latter.  Adoption becomes much more reasonable once users realize the value of investing in learning a new skill.  

Advocates of open science also recognize the limitations of teaching in that not all audiences can be reached and not all materials are retained or even used after training.  A technique of letting the trainee become the trainer can be used whereby those that successfully learn new skills can teach others at their home institutions.  Those that also adopt new workflows through training can also direct their research products to facilitate collaboration with non-adopters rather than the latter synthesizing and analyzing their data in potentially suboptimal ways [@Touchon16]. These "champions" can be a voice of encouragement for others by demonstrating how new tools can be introduced and learned over time through shared experiences [@Lowndes17].  This also encourages the development of a community of practice that shares and learns together to navigate the collection of existing and developing open science tools.  

The scientific culture within a discipline may inhibit the adoption of open science methods.  A common argument against open science is the protection of data that an individual research team may view as proprietary or sensitive.  There are reasonable arguments to treat data as personal property, particularly if exceptional effort was spent to secure funding for a project and if the data were hard-earned or sensitive, e.g., detailed location data on endangered species or medical/socioeconomic data.  These issues are less of a concern for bioassessment where many datasets are collected by institutions that are publicly funded and data accessibility may be mandated by law.  However, an open science process dictates that both interim and completed research products derived from public data should be available to the broader bioassessment community.  This raises an additional concern that research teams using transparent workflows could expose themselves to increased criticism by their peers and the public [@Lewandowsky16], particularly where the developed products can have important regulatory implications.  However, an argument made throughout this paper is that regulatory, management, and stakeholder groups that will both use and be affected by bioassessment products should be integral contributors to the development process.  An open science bioassessment process welcomes criticism and feedback as a natural part of development that will facilitate adoption by ensuring the product meets the needs of all parties.  Ultimately, these products are developed to improve the environment as a public resource.  The ideals promoted by an open science process directly align with these goals.

Institutional barriers can inhibit open science given the scale of change that must occur for adoption. Bureaucratic hurdles can disincentivize initiatives that promote change, particularly if that change originates from researchers not in administrative roles.  Regulatory institutions may also prefer some level of opacity for how research products that influence policy are made available during development.  The level of transparency advocated by open science could be viewed as opening the floodgates to increased legal scrutiny that can unintentionally hinder forward progress.  Despite these reservations, many public institutions now advocate for increased openness because of the benefits that facilitate and engender public trust.  Open data initiatives are now fairly common and represent a form of advocacy by public institutions for broader adoption of open science principles.  Although these changes represent progress, many institutions have yet to strictly define open science and how it is applied internally and externally.  As open science continues to build recognition, means of integrating toolsets that promote openness and transparency beyond publicly shared data will have to be adopted by regulatory and management institutions. 

# Conclusions

The relevance of bioassessment applications can be improved with open science by using reproducible, transparent, and effective tools that bridge the gap between research and management.  Many open science tools can improve communication between researchers and managers to expose all aspects of the research process and facilitate implementation to support policy, regulation, or monitoring efforts.  Communication ensures that the developed product is created through an exchange of ideas to balance the potentially competing needs of different sectors and institutions.  The documentation and archiving of data used to create a bioassessment product also ensures that other researchers can discover and build on past efforts, rather than constantly rebuilding the wheel.  Incremental improvements of existing products can reduce the proliferation of site- and taxon-specific methods with limited regional applications by exploring new ways to integrate biological indicators across space and time.   

# Author contributions

MB and RM performed the research in the case study. All authors contributed to the conceptual development and writing of the manuscript.  

# Acknowledgments

The authors acknowledge support from the San Gabriel River Regional Monitoring Program and the Sanitation Districts of Los Angeles County.  We thank Eric Stein for reviewing an earlier draft of the manuscript. We thank Mike McManus for assistance with the conceptual diagrams.   

# Figures 

```{r basicflow, fig.cap = 'A simplified workflow of the open science paradigm (adapted from @Hampton15).  All aspects of the research process, from the conception of an idea to publishing a product, can be enhanced using open science tools.  The workflow is iterative where products are continually improved through collaborations facilitated through discovery and reproducibility of open data.'}
knitr::include_graphics('figs/basicflow.png')
```

```{r closedopen, fig.cap = 'Examples of (a) closed and (b) open approaches for bioassessment applications. The green box in each subfigure represents the technical steps of the individual research team for developing the product, the manager and stakeholder box are those that require or motivate the creation of bioassessment products, the gray boxes indicate sources of external information (data and guidance documents) as input into the technical process, and the open text indicates open components of the planning, translation, or implementation phase of a bioassessment product. Figures were adapted from @Hampton15. NGO: non-government organization, RMP: regional monitoring program.'}
knitr::include_graphics('figs/closedopen.png')
```

```{r scapex, message = F, results = 'hide'}
######
# land use map

thrsh <- 0.79; tails <- 0.1

# land use colors
lucol <- c('grey20', 'grey40', 'grey60', 'khaki3', 'khaki2')

# nhd sgr str class
sgrcls <- getcls2(spat, thrsh = thrsh, tails = tails, modls = 'core') %>% 
  left_join(spat, ., by = 'COMID') %>% 
  dplyr::select(COMID, strcls)

# get biological condition expectations
cls <- getcls2(spat, thrsh = thrsh, tails = tails, modls = 'core') %>% 
  mutate(COMID = as.character(COMID)) 

# hydrolines
spatdat <- spat %>% 
  dplyr::select(COMID) %>% 
  mutate(id = as.character(1:nrow(.)))
st_geometry(spatdat) <- NULL
spatfrt <- spat %>% 
  as('Spatial') %>% 
  fortify %>% 
  left_join(spatdat, by = 'id') %>% 
  mutate(COMID = as.character(COMID)) %>% 
  left_join(cls, by = 'COMID') %>% 
  mutate(
    strcls = factor(strcls, levels = levels(strcls))
  ) %>% 
  filter(!is.na(strcls))

# get csci difference
sgrscrs <- spat %>% 
  dplyr::select(COMID, core0.50) %>% 
  mutate(COMID = as.character(COMID)) %>% 
  left_join(scrs, ., by = 'COMID') %>% 
  group_by(COMID, StationCode, lat, long) %>% 
  summarise(
    csci = mean(csci, na.rm = TRUE)
  ) %>% 
  ungroup %>%
  left_join(cls, by = 'COMID')

# wshed boundary fortified
shedfrt <- shed %>% 
  as('Spatial') %>% 
  fortify 

# relative site scores
sgrexp <- site_exp(spat, sgrscrs, thrsh = thrsh, tails = tails, modls = 'core') %>% 
  dplyr::select(-data, -datcut) %>% 
  mutate(
    strcls = factor(strcls, levels = levels(strcls)),
    perf = factor(perf, levels = rev(levels(perf))),
    typelv = as.character(typelv)
  ) %>% 
  filter(!is.na(perf))

# land use raster data
luplo <- sgrlu %>% 
  rasterToPoints %>% 
  data.frame %>% 
  dplyr::rename(`Land cover` = layer) %>% 
  mutate(`Land cover` = factor(`Land cover`, levels = c(5, 4, 3, 2, 1), labels = c('Urban: hi', 'Urban: md', 'Urban: lo', 'Open: forest', 'Open: chaparral'))) 

# separate lu legend
luleg <- ggplot(luplo) + 
  geom_tile(aes(x, y, fill = `Land cover`), alpha=0.8) +
  theme_bw(base_family = 'serif') +
  theme(
    legend.position = 'top', 
    legend.justification = 'left',
    legend.box.just = 'left',
    legend.box = 'vertical'
  ) +  
  scale_fill_manual('Land cover', values = lucol) +
  guides(fill = guide_legend(ncol = 2, title.position = 'top'))
luleg <- g_legend(luleg)

# land use colors
lucol <- c('grey20', 'grey40', 'grey60', 'khaki3', 'khaki2')

# land use plot
p1 <- ggplot(luplo) + 
  geom_tile(aes(x, y, fill = `Land cover`), alpha=0.5) +
  theme_void(base_family = 'serif') +
  scale_fill_manual('Land cover', values = lucol) +
  # scale_colour_manual('Stream segment class', values = pal_exp(levels(spatfrt$strcls)), guide = F) +
  # geom_point(data = sgrexp, aes(x = long, y = lat, colour = strcls, shape = perf), size = 3, alpha = 0.9) +
  # scale_shape_manual('', values = c(24, 21, 25)) +
  # geom_path(data = spatfrt, aes(x = long, y = lat, group = id, colour = strcls), size = 0.5) +
  # geom_polygon(data = shedfrt, aes(x = long, y = lat, group = group), fill = NA, colour = 'black', alpha = 0.7) +
  coord_equal() +
  theme_void(base_family = 'serif') +
  theme(
    legend.position = 'none', 
    axis.title = element_blank()
  )

# land use plot
p2 <- ggplot() +
  theme_void(base_family = 'serif') +
  scale_colour_manual('Stream segment class', values = pal_exp(levels(spatfrt$strcls)), guide = F) +
  scale_shape_manual('', values = c(24, 21, 25)) +
  geom_path(data = spatfrt, aes(x = long, y = lat, group = id, colour = strcls), size = 0.5) +
  geom_point(data = sgrexp, aes(x = long, y = lat, fill = strcls, shape = perf), size = 3, alpha = 0.9) +
  geom_polygon(data = shedfrt, aes(x = long, y = lat, group = group), fill = NA, colour = 'black', alpha = 0.7) +
  scale_fill_manual(values = pal_exp(levels(sgrexp$strcls)), na.value = 'yellow', guide = F) +
  coord_equal() +
  theme_void(base_family = 'serif') +
  theme(
    legend.position = 'none', 
    axis.title = element_blank(),
    panel.background = element_rect(fill = "transparent"),
    plot.background = element_rect(fill = "transparent", color = NA) 
  )

png('figs/lumap.png', height = 7, width = 4, units = 'in', res = 300, family = 'serif')
p1
dev.off()

png('figs/clmap.png', height = 7, width = 4, units = 'in', res = 300, family = 'serif')
p2
dev.off()

png('figs/luleg.png', height = 2, width = 2, units = 'in', res = 300, family = 'serif')
grid.arrange(arrangeGrob(luleg))
dev.off()

#######
# ggplot of sgr site expecations, relative scores

# requires spat, scrs

thrsh <- 0.79; tails <- 0.1

## get sgr expectation for ggplot
# process
incl <- site_exp(spat, sgrscrs, thrsh = thrsh, tails = tails, modls = 'core') %>% 
  dplyr::select(-lat, -long) %>% 
  group_by(StationCode) %>% 
  nest

# average scors by station
out <- sgrscrs %>% 
  dplyr::select(StationCode, lat, long) %>% 
  group_by(StationCode) %>% 
  nest %>% 
  mutate(StationCode = factor(StationCode, levels = levels(incl$StationCode))) %>% 
  left_join(incl, ., by = 'StationCode') %>% 
  unnest

# add additional perf column for multicolor by strcls (pal_prf)
# remove NA csci
ggsgrexp <- get_perf_mlt(out) %>% 
  filter(!is.na(strcls))

# CSCI scores and expectations
toplo1 <- ggsgrexp %>%
  dplyr::select(COMID, StationCode, datcut, strcls, csci, perf, typelv, perf_mlt) %>%
  unnest %>%
  mutate(
    strcls = factor(strcls, levels = c("likely constrained", "possibly constrained", "possibly unconstrained", "likely unconstrained")),
    perf = factor(perf, levels = levels(perf))
  ) %>%
  dplyr::rename(
    `Stream Class` = strcls,
    `Relative\nscore` = perf_mlt,
    Type = typelv
  )

# total expected range
toplo2 <- ggsgrexp %>%
  dplyr::select(COMID, StationCode, data, strcls) %>%
  unnest %>%
  mutate(strcls = factor(strcls, levels = levels(strcls))) %>%
  dplyr::rename(`Stream Class` = strcls)

# median expectation
toplo3 <- ggsgrexp %>%
  dplyr::select(COMID, StationCode, datcut) %>%
  unnest %>%
  filter(grepl('0\\.50$', var)) 

# plot, context
p3 <- ggplot(toplo1, aes(y = StationCode, x = val)) +
  geom_line(data = toplo1, aes(x = val, colour = `Stream Class`), alpha = 0.1, size = 2) +
  geom_line(aes(colour = `Stream Class`), alpha = 0.6, size = 2) +
  geom_point(data = toplo3, colour = 'white', size = 1, alpha = 1, shape = 15) +
  theme_bw(base_family = 'serif', base_size = 12) +
  theme(
    axis.text.y = element_blank(),
    # axis.title.y = element_blank(),
    # axis.title.x = element_blank(), 
    # axis.ticks.y = element_blank(), 
    legend.position = 'top', 
    title = element_text(size = 10)
  ) +
  scale_x_continuous('CSCI') +
  scale_y_discrete('Site') +
  scale_colour_manual('Segment class', values = pal_exp(levels(toplo1$`Stream Class`)), drop = F) +
  geom_point(aes(x = csci, fill = `Stream Class`, shape = perf), size = 2.5, alpha = 0.8) +
  geom_vline(xintercept = thrsh, linetype = 'dashed', size = 1) +
  scale_shape_manual('Site score', values = c(25, 21, 24)) +
  scale_fill_manual(values = pal_exp(levels(toplo1$`Stream Class`)), na.value = 'yellow', guide = F) +
  guides(shape = guide_legend(title.position = 'top'), colour = guide_legend(title.position = 'top', ncol = 2))
p3leg <- g_legend(p3)
p3 <- p3 + theme(legend.position = 'none')

png('figs/sgrexp.png', height = 7, width = 4, units = 'in', res = 300, family = 'serif')
p3
dev.off()

png('figs/sgrexpleg.png', height = 1, width = 7, units = 'in', res = 300, family = 'serif')
grid.arrange(arrangeGrob(p3leg))
dev.off()
```
```{r scapexfig, fig.cap = 'Schematic demonstrating how the Stream Classification and Community Explorer (SCAPE) can be used to identify potential management actions for stream sites.  Stream segment classifications are defined as biologically constrained or unconstrained based on landscape characteristics (left map) and sites with bioassessment scores are evaluated relative to the classifications.  Sites can be under-scoring, as expected, or over-scoring relative to the segment classification and expected range of scores (middle plot).  Unconstrained sites are those where present landscape conditions do not limit biological potential and constrained sites are those where landscape conditions limit biological potential (right images).  Management actions and priorities can be defined based on site scores relative to segment classifications.'}
knitr::include_graphics('figs/scapex.png')
```

```{r scapeopensci, fig.cap = 'The open science workflow for creating the SCAPE tool.  The approach was iterative following those shown in Figures \\@ref(fig:basicflow) and \\@ref(fig:closedopen)b to meet stakeholder needs (box 1; San Gabriel River Regional Monitoring Program, Sanitation District of Los Angeles County, Southern California Coastal Water Research Project).  Specific open science or open source applications used at each step are shown in each box with publicly accessible URL links where appropriate.'}
knitr::include_graphics('figs/scapeopensci.png')
```


# Tables

```{r osprinc}
tab <- data.frame(
  `Concepts and principles` = c(
    'Open',
    'Open Science',
    'Principle 1', 
    'Principle 2', 
    'Principle 3', 
    'Principle 4'
    ),
  Description = c(
    'Anyone can freely access, use, modify, and share for any purpose', 
    'The practice of science in such a way that others can collaborate and contribute, where research data, lab notes and other research processes are freely available, under terms that enable reuse, redistribution and reproduction of the research and its underlying data and methods',
    'Transparency in experimental methodology, observation, and collection of data', 
    'Public availability and reusability of scientific data', 
    'Public accessibility and transparency of scientific communication', 
    'Using web-based tools to facilitate scientific collaboration and reproducibility'
    ),
  # footnote = c(
  #   'Open Knowledge International, http://opendefinition.org/', 
  #   'Adopted by Creative Commons, https://creativecommons.org/about/program-areas/open-science/', 
  #   'J. Gezelter, https://www.openscience.org',
  #   '', 
  #   '', 
  #   ''
  # ), 
  stringsAsFactors = F
)

# table stuff
cap.val <- 'Core definitions and principles of open science, from Open Knowledge International, http://opendefinition.org/, Creative Commons, https://creativecommons.org/about/program-areas/open-science/, and J. Gezelter, https://www.openscience.org'

# table
knitr::kable(tab, booktabs = T, caption = cap.val)
```


```{r rpkgtab}
tab <- data.frame(
  Task = c(
    "General", 
    "Mapping, geospatial", 
    "", 
    "",
    "",
    "Statistical modelling", 
    "", 
    "", 
    "Community analysis", 
    "", 
    "", 
    "Science communication", 
    "", 
    ""
    ), 
  Package = c(
    "`tidyverse` (Wickham 2017)", 
    "`sf` (Pebesma 2018)", 
    "`raster` (Hijmans 2019)",
    "`leaflet` (Cheng et al. 2018)", 
    "`mapview` (Appelhans et al. 2018)", 
    "`randomForest` (Liaw and Wiener 2002)", 
    "`nlme` (Pinheiro et al. 2018)", 
    "`mgcv` (Wood 2017)", 
    "`TITAN2` (Baker et al. 2015)", 
    "`indicspecies` (De Caceres and Legendre 2009)", 
    "`vegan` (Oksanen et al. 2018)", 
    "`shiny` (Chang et al. 2018)", 
    "`rmarkdown` (Allaire et al. 2018)", 
    "`knitr` (Xie 2015)"
    ), 
  Description = c(
    "A suite of packages to import, wrangle, explore, and plot data.  Includes the popular `ggplot2` and `dplyr` packages.", 
    "A simple features architecture for working with vectorized spatial data, including common geospatial analysis functions", 
    "Reading, writing, manipulating, analyzing, and modeling gridded spatial data", 
    "Integration of R with the popular JavaScript `leaflet` library for interactive maps", 
  "Creates interactive maps to quickly examine and visually investigate spatial data, built off `leaflet` and integrated with `sf`", 
    "Create classification and regression trees for predictive modelling", 
    "Non-linear, mixed effects modelling", "Generalized additive modelling", 
    "Ecological community threshold analysis using indicator species scores", 
    "Indicator species analysis", "Multivariate analysis for community ecology", 
    "Reactive programming tools to create interactive and customizable web applications", 
    "Tools for working with markdown markup languages in .Rmd files", 
    "Automated tools for markdown files that process integrated R code chunks"
  ), 
  stringsAsFactors = F
)

# table stuff
cap.val <- 'R packages that can be used in the development, application, and translation of bioassessment products.'

# table
knitr::kable(tab, booktabs = T, caption = cap.val)
```

# References

